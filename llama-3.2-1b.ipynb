{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import huggingface_hub\n",
    "huggingface_hub.login(\"xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083293e3c83d42fdb0c0264f900604dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\study64\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\양진우\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0368793a73984f03aef2b92b05aa6418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccccd13ee9cb453da9e4254d9076116c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f35f6e9e138474ea6bb4e0251765671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0109e8c8384d46e8a974398db16dc959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbdc3d8bd0949cb9998368903517dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which is a fruit? (a) apple, (b) car, (c) dog, (d) apple\n",
      "A. b\n",
      "B. d\n",
      "C. c\n",
      "D. a\n",
      "Answer: D\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "prompt = \"Which is a fruit? (a) apple, (b) car, (c) dog\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=10\n",
    "        )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 정의\n",
    "test_questions = [\n",
    "    \"Which is a furniture? (a) chair, (b) dog, (c) pencil\",\n",
    "    \"Which is a planet? (a) Mars, (b) Moon, (c) Sun\",\n",
    "    \"Which is a color? (a) happy, (b) blue, (c) fast\",\n",
    "    \"Which is a vehicle? (a) banana, (b) car, (c) tree\",\n",
    "    \"Which is an animal? (a) house, (b) phone, (c) lion\",\n",
    "    \"Which is a vegetable? (a) carrot, (b) book, (c) chair\",\n",
    "    \"Which is a drink? (a) table, (b) water, (c) shoe\",\n",
    "    \"Which is a sport? (a) sleep, (b) eat, (c) tennis\",\n",
    "    \"Which is a weather? (a) rainy, (b) smile, (c) walk\",\n",
    "    \"Which is a job? (a) doctor, (b) cloud, (c) song\"\n",
    "]\n",
    "\n",
    "test_answers = ['a', 'a', 'b', 'b', 'c', 'a', 'b', 'c', 'a', 'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: Which is a furniture? (a) chair, (b) dog, (c) pencil\n",
      "모델 답변: Which is a furniture? (a) chair, (b) dog, (c) pencil, (d) bed.\n",
      "Which is not a furniture? (a) table, (b) door, (c) lamp, (d) bookcase.\n",
      "What is a furniture? (a) chair, (b) dog, (c) pencil, (d) bed.\n",
      "What is not a furniture? (a) table, (b) door, (c) lamp, (d) bookcase.\n",
      "Which is a furniture? (a) chair, (b) dog, (c) pencil, (d) bed.\n",
      "Which is not a furniture? (a) table, (b) door, (c) lamp, (d) bookcase.\n",
      "What is a furniture? (a) chair, (b) dog, (c) pencil, (d) bed.\n",
      "What is not a furniture? (a) table, (b) door, (c) lamp, (d) bookcase.\n",
      "What is a furniture? (a) chair, (\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: Which is a planet? (a) Mars, (b) Moon, (c) Sun\n",
      "모델 답변: Which is a planet? (a) Mars, (b) Moon, (c) Sun, (d) Jupiter.\n",
      "Which is a satellite? (a) Moon, (b) Mars, (c) Venus, (d) Earth.\n",
      "Which is a moon? (a) Moon, (b) Earth, (c) Venus, (d) Jupiter.\n",
      "Which is a planet? (a) Earth, (b) Sun, (c) Moon, (d) Mars.\n",
      "Which is a satellite? (a) Sun, (b) Mars, (c) Moon, (d) Venus.\n",
      "Which is a moon? (a) Earth, (b) Mars, (c) Sun, (d) Moon.\n",
      "Which is a planet? (a) Moon, (b) Sun, (c) Earth, (d) Venus.\n",
      "Which is a satellite? (a) Earth, (b) Sun, (c) Moon, (d) Mars.\n",
      "Which is a moon? (a) Sun, (b) Moon, (c) Venus\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: Which is a color? (a) happy, (b) blue, (c) fast\n",
      "모델 답변: Which is a color? (a) happy, (b) blue, (c) fast, (d) red, (e) yellow\n",
      "Answer: a\n",
      "Explanation: A color is a quality of light that makes some objects stand out from their surroundings. The quality of color is the same regardless of which object you are comparing the color to.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: Which is a vehicle? (a) banana, (b) car, (c) tree\n",
      "모델 답변: Which is a vehicle? (a) banana, (b) car, (c) tree, (d) truck, (e) plane\n",
      "Which is a vehicle? (a) banana, (b) car, (c) tree, (d) truck, (e) plane\n",
      "Which is aToSend a message to\n",
      "Which is a vehicle? (a) banana, (b) car, (c) tree, (d) truck, (e) plane\n",
      "Which is a vehicle? (a) banana, (b) car, (c) tree, (d) truck, (e) plane\n",
      "Which is a vehicle? (a) banana, (b) car, (c) tree, (d) truck, (e) plane\n",
      "Which is a vehicle? (a) banana, (b) car, (c) tree, (d) truck, (e) plane\n",
      "Which is a vehicle? (a) banana, (b) car, (c) tree, (d) truck, (e) plane\n",
      "Which\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: Which is an animal? (a) house, (b) phone, (c) lion\n",
      "모델 답변: Which is an animal? (a) house, (b) phone, (c) lion, (d) dog\n",
      "Which is an animal? (a) house, (b) phone, (c) lion, (d) dog\n",
      "Which is an animal? (a) house, (b) phone, (c) lion, (d) dog\n",
      "Which is an animal? (a) house, (b) phone, (c) lion, (d) dog Francesca is learning about animals. Which is an animal? (a) house, (b) phone, (c) lion, (d) dog\n",
      "Which is an animal? (a) house, (b) phone, (c) lion, (d) dog\n",
      "Which is an animal? (a) house, (b) phone, (c) lion, (d) dog\n",
      "Which is an animal? (a) house, (b) phone, (c) lion, (d) dog\n",
      "Which is an animal? (a) house, (b)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: Which is a vegetable? (a) carrot, (b) book, (c) chair\n",
      "모델 답변: Which is a vegetable? (a) carrot, (b) book, (c) chair, (d) car\n",
      "A. c\n",
      "B. d\n",
      "C. b\n",
      "D. a\n",
      "Answer: C\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: Which is a drink? (a) table, (b) water, (c) shoe\n",
      "모델 답변: Which is a drink? (a) table, (b) water, (c) shoe, (d) cup, (e) bottle\n",
      "Answer: d\n",
      "Explanation: A drink is a liquid that is usually consumed with food. A cup of water is a drink, but a shoe is not.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: Which is a sport? (a) sleep, (b) eat, (c) tennis\n",
      "모델 답변: Which is a sport? (a) sleep, (b) eat, (c) tennis, (d) walk\n",
      "Which is a sport? (a) sleep, (b) eat, (c) tennis, (d) walk\n",
      "The answer is (a) sleep.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: Which is a weather? (a) rainy, (b) smile, (c) walk\n",
      "모델 답변: Which is a weather? (a) rainy, (b) smile, (c) walk, (d) snow,copy and paste the word in the box. Which is a weather? (a) rainy, (b) smile, (c) walk, (d) snow.\n",
      "Which is a weather? (a) rainy, (b) smile, (c) walk, (d) snow,copy and paste the word in the box. Which is a weather? (a) rainy, (b) smile, (c) walk, 형태소 분석서 (d) snow.\n",
      "Which is a weather? (a) rainy, (b) smile, (c) walk, (d) snow,copy and paste the word in the box. Which is a weather? (a) rainy, (b) smile, (c) walk, (d) snow.\n",
      "Which is a weather? (a) rainy, (b) smile, (c) walk, (d) snow,copy and paste the word in the box. Which is a weather? (a\n",
      "\n",
      "질문: Which is a job? (a) doctor, (b) cloud, (c) song\n",
      "모델 답변: Which is a job? (a) doctor, (b) cloud, (c) song, (d) dog.\n",
      "A. a\n",
      "B. b\n",
      "C. c\n",
      "D. d\n",
      "Answer: A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예측 실행\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "for question in test_questions:\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=10\n",
    "        )\n",
    "    \n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predictions.append(prediction)\n",
    "    print(f\"질문: {question}\")\n",
    "    print(f\"모델 답변: {prediction}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study64",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
